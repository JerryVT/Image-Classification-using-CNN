{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m5JjP1w4lWsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "m72Opmc7lcE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://dl.dropboxusercontent.com/s/w9aqbqxmj4i2my8/dataset.zip\"\n",
        "!unzip dataset.zip\n",
        "!ls"
      ],
      "metadata": {
        "id": "s7mIK54m4thP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries"
      ],
      "metadata": {
        "id": "LXTa1-WKlefy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIkA5ktfkSSG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6oZiKWrvlaxG",
        "outputId": "f29d8eda-8053-49c2-9b80-4ba0385a31a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Data Preprocessing"
      ],
      "metadata": {
        "id": "g1RgkudR380Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the training set"
      ],
      "metadata": {
        "id": "TQkd5Bas4B8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to do some trasnformations as preprocessing on training set for reducing overfitting\n",
        "\n",
        "# For this some geometric trasnformations are applied such as zoom, rotations, shift pixels horizontal flips and some zoom in and zoom out\n",
        "# The tecnical term for transforming the images of training set is called Image Augmentation\n",
        "# This will prevent CNN from overlearning or overfitting\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                  rescale=1./255,     # performs feature scaling and acts as standardization to get values between 0 and 1 as there are 255 values for color\n",
        "                  shear_range=0.2,\n",
        "                  zoom_range=0.2,\n",
        "                  horizontal_flip=True\n",
        ")\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'dataset/training_set', # path to training set\n",
        "    target_size=(64,64), # final size of image set which is going to be fed to CNN. larger values takes more time\n",
        "    batch_size=32, # how amny images in each batch. 32 is a common value. making batches reduces the load on machine\n",
        "    class_mode= 'binary' # specifies either binary or categorical. as now we have to choose betrwenn cat and dog hence binary \n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "kzjxVDwZ32g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4ef793-2a6f-4e85-cf7d-b76ec8ad74f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the test set"
      ],
      "metadata": {
        "id": "VxAoplYf8iT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./200) # we are not going to apply the transformations we did on training set as they are new images \n",
        "                                # which are going to passed when deploying and need to keep them intact. \n",
        "                                # But rescaling must be done (i.e feature scaling as both training sand test set should be in same scale)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    'dataset/test_set',\n",
        "    target_size=(64,64),   # same size as used for training set\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "0cItw6A78kX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9863f638-a9e4-49d1-d719-a94fccf3b3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 : Building the CNN"
      ],
      "metadata": {
        "id": "gAGo5dvP9Gu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialising the CNN"
      ],
      "metadata": {
        "id": "tzZg1HNX9L4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()    # to initilize cnn variable as sequence of layers. other is computational graph"
      ],
      "metadata": {
        "id": "5MvfFFhG9JuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Convolution"
      ],
      "metadata": {
        "id": "iglOJtiR9gac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the convolutional layer going to be added below using add method is an object of class Conv2D\n",
        "# this class acts as the dense class which allows us to build fully connected layer\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(       #  to add concolution layer\n",
        "    filters = 32,                      # no of feture detectors (also called filters or kernels) which need to be applied,\n",
        "    kernel_size = 3,                   # size of feature detector/kernel i.e size of row/ col as it is a sq matrix\n",
        "    activation = 'relu',\n",
        "    input_shape = [64,64,3]         # as we have resized the image to 64 x 64, it should be the size. and since it is color, we need to specify channel as 3. if grayscale it should be 1    \n",
        "    ))"
      ],
      "metadata": {
        "id": "WKbeJCOg9rUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : Pooling"
      ],
      "metadata": {
        "id": "pSWq-gr_9f2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying max pooling (refer doc)\n",
        "# as we are adding pooling layer to our CNN, we need to call add method which is used to add layers in NN\n",
        "\n",
        "# here also we are passing an obkect of class Maxpool2D belong to layers of tf.keras as shown above for max pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(\n",
        "      pool_size = 2,          # size of frame which should be considered which is asq matrix (refer doc for ex, same is considered here)\n",
        "      strides = 2,        # specifies no of  pixels for frame to shift. it acts as an offset for jumping of frame from one block to next block. 2 means after 1 ele, it jusmps to 3rd ele\n",
        "      # check other args also\n",
        "))"
      ],
      "metadata": {
        "id": "K10PAsNeAarh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding a second Convolution Layer"
      ],
      "metadata": {
        "id": "c6y7b4yECU5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to do the above steps again for second concplution layer\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(       #  to add concolution layer\n",
        "    filters = 32,                      # no of feture detectors (also called filters or kernels) which need to be applied,\n",
        "    kernel_size = 3,                   # size of feature detector/kernel i.e size of row/ col as it is a sq matrix\n",
        "    activation = 'relu',\n",
        "    # we dont need to specify input_size, its needed only for first time to connect automatically connect input layer to first layer0 \n",
        "    ))\n",
        "\n",
        "cnn.add(tf.keras.layers.MaxPool2D(\n",
        "      pool_size = 2,          # size of frame which should be considered which is asq matrix (refer doc for ex, same is considered here)\n",
        "      strides = 2,        # specifies no of  pixels for frame to shift. it acts as an offset for jumping of frame from one block to next block. 2 means after 1 ele, it jusmps to 3rd ele\n",
        "      # check other args also\n",
        "))"
      ],
      "metadata": {
        "id": "BdUKl8CJCanK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flattening"
      ],
      "metadata": {
        "id": "dwRzGgYFEGJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for flattening we again to need to call object of a class within add method of cnn object\n",
        "# keras will automatically understand the input to flatten is output of all the convolutions and pooling which is strord in cnn object\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten()) # no need to pass any args"
      ],
      "metadata": {
        "id": "zhucxVlDEKCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : Full Connection"
      ],
      "metadata": {
        "id": "csALLdVuETwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creatin fully connected layer to the above flatten layer\n",
        "# remember from here, it is same as implementation of ANN\n",
        "\n",
        "cnn.add(tf.keras.layers.Dense(units = 128,  # larger than one we taken for ANN as we are doing complex computation\n",
        "                              activation = 'relu'\n",
        "                              ))"
      ],
      "metadata": {
        "id": "gG1IZtiuE9cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 : Output Layer"
      ],
      "metadata": {
        "id": "SavB19cxFWP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units = 1,  # as we are doing bianry classification and hence one class is needed\n",
        "                              activation = 'sigmoid' # softmax for multiclass \n",
        "                              ))"
      ],
      "metadata": {
        "id": "ia88AK_HFa2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 : Training the CNN"
      ],
      "metadata": {
        "id": "nDkWwRTzGM3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling the CNN"
      ],
      "metadata": {
        "id": "lY_rmK08GXrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling CNN here is same as that of ANN as we are deling with binary\n",
        "\n",
        "cnn.compile(optimizer = 'adam',\n",
        "            loss = 'binary_crossentropy',\n",
        "            metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "oifipdOFGSpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the CNN on Training set and evaluating it on test set"
      ],
      "metadata": {
        "id": "6NutfxrwG3di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "id": "OfuU2ILXG9b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1aaec5-c882-4044-ebdc-ca68a07c04ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 46s 145ms/step - loss: 0.6943 - accuracy: 0.5199 - val_loss: 0.6742 - val_accuracy: 0.5970\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.6555 - accuracy: 0.6186 - val_loss: 0.6530 - val_accuracy: 0.6325\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.6065 - accuracy: 0.6696 - val_loss: 0.6692 - val_accuracy: 0.6390\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.5713 - accuracy: 0.7036 - val_loss: 0.5992 - val_accuracy: 0.6960\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.5424 - accuracy: 0.7226 - val_loss: 0.5512 - val_accuracy: 0.7385\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 37s 146ms/step - loss: 0.5218 - accuracy: 0.7336 - val_loss: 0.5706 - val_accuracy: 0.7270\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 36s 146ms/step - loss: 0.4994 - accuracy: 0.7516 - val_loss: 0.6088 - val_accuracy: 0.7215\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 37s 147ms/step - loss: 0.4746 - accuracy: 0.7699 - val_loss: 0.5616 - val_accuracy: 0.7335\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 36s 146ms/step - loss: 0.4613 - accuracy: 0.7786 - val_loss: 0.6260 - val_accuracy: 0.7410\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 37s 147ms/step - loss: 0.4489 - accuracy: 0.7840 - val_loss: 0.5897 - val_accuracy: 0.7420\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 37s 148ms/step - loss: 0.4344 - accuracy: 0.7937 - val_loss: 0.5122 - val_accuracy: 0.7720\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.4174 - accuracy: 0.8045 - val_loss: 0.5484 - val_accuracy: 0.7530\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 37s 147ms/step - loss: 0.4024 - accuracy: 0.8127 - val_loss: 0.5808 - val_accuracy: 0.7595\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.3865 - accuracy: 0.8183 - val_loss: 0.4860 - val_accuracy: 0.7775\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.3703 - accuracy: 0.8345 - val_loss: 0.5432 - val_accuracy: 0.7755\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.3560 - accuracy: 0.8434 - val_loss: 0.5858 - val_accuracy: 0.7695\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.3390 - accuracy: 0.8471 - val_loss: 0.6145 - val_accuracy: 0.7605\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.3219 - accuracy: 0.8577 - val_loss: 0.6057 - val_accuracy: 0.7870\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.3020 - accuracy: 0.8681 - val_loss: 0.5726 - val_accuracy: 0.7825\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.2823 - accuracy: 0.8814 - val_loss: 0.6451 - val_accuracy: 0.7670\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.2701 - accuracy: 0.8899 - val_loss: 0.6245 - val_accuracy: 0.7825\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.2571 - accuracy: 0.8963 - val_loss: 0.6200 - val_accuracy: 0.7820\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.2404 - accuracy: 0.9029 - val_loss: 0.8021 - val_accuracy: 0.7590\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.2237 - accuracy: 0.9094 - val_loss: 0.8014 - val_accuracy: 0.7530\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 36s 146ms/step - loss: 0.2152 - accuracy: 0.9145 - val_loss: 0.7295 - val_accuracy: 0.7800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc93581ccd0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 : Making a single Prediction"
      ],
      "metadata": {
        "id": "A3a7Ru8DHgBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',\n",
        "                           target_size= (64, 64)  # we need to change the size of img as that of input\n",
        "                           )\n",
        "# We need to do some extraworks\n",
        "\n",
        "# we need to change pil format (format of image) to a numpy array\n",
        "test_image = image.img_to_array(test_image)\n",
        "\n",
        "# predict method has to be called on precise format\n",
        "# bacth size was mentioned so CNN is not trained to work on a single image\n",
        "# so test image need to be in a batch of 32\n",
        "\n",
        "test_image = np.expand_dims(test_image, axis = 0) # this makes the dimension to be added in first row as we need batch size 32 to be mentioned in first row\n",
        "result = cnn.predict(test_image)\n",
        "\n",
        "# but the output we get is either 0 or 1.\n",
        "# we need to know whther dog is 0 or 1 and cat is 0 or 1\n",
        "training_set.class_indices\n",
        "# from here we get 1 corresponds to Dog\n",
        "\n",
        "if result[0][0] == 1: # first 0 value indicates first index of batch 32. this is becasue we only have one image in batch of 32\n",
        "                      # second 0 indicates the predicted output\n",
        "  prediction = 'Dog'\n",
        "else:\n",
        "  prediction = 'Cat'\n",
        "\n"
      ],
      "metadata": {
        "id": "UDf-IoHGHpfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "id": "8FLmpNuOOX-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f695d4f-3b43-4664-a6ae-b2b345bc225d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg',\n",
        "                           target_size= (64, 64)  # we need to change the size of img as that of input\n",
        "                           )\n",
        "\n",
        "test_image = image.img_to_array(test_image)\n",
        "\n",
        "test_image = np.expand_dims(test_image, axis = 0) # this makes the dimension to be added in first row as we need batch size 32 to be mentioned in first row\n",
        "result = cnn.predict(test_image)\n",
        "\n",
        "\n",
        "training_set.class_indices\n",
        "\n",
        "\n",
        "if result[0][0] == 1: # first 0 value indicates first index of batch 32. this is becasue we only have one image in batch of 32\n",
        "                      # second 0 indicates the predicted output\n",
        "  prediction = 'Dog'\n",
        "else:\n",
        "  prediction = 'Cat'\n",
        "\n"
      ],
      "metadata": {
        "id": "A_kPkNRz9Tuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "id": "1l0NHknD9fwn",
        "outputId": "f3376e8e-b77c-4014-fb02-e3e955c26b19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat\n"
          ]
        }
      ]
    }
  ]
}